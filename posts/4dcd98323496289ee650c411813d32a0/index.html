<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>自然语言处理—RNN循环神经网络 - 追风少年的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="自然语言处理—RNN循环神经网络" />
<meta property="og:description" content="1. RNN基本介绍 (参见吴恩达深度学习—RNN篇https://www.bilibili.com/video/BV16r4y1Y7jv?p=152&amp;spm_id_from=pageDriver)
1.1 基本介绍 RNN神经网络可以用来处理时间序列类型的数据，一段文字也可以看成一个时间序列。比如，
The cat ate many food that was delicious was full
其中，上标代表第个样本，代表时间，下面将不再样本的标记进行分析。一般的神经网络处理时间序列时存在两个问题：i) 输入维度不确定，且如果是one-hot编码，则向量维度高；ii) 一般神经网络不能捕捉序列之间的信息。因此，有循环神经网络来处理时间序列数据。
图1. RNN神经网络模型结果
前向传播
在RNN中，每一时刻所使用的参数、、以及、 、都是一致的。模型的forward-propagation如下：
（1）
其中，。
反向传播
每一个时刻，都有损失函数：
总的损失为。其back-propagation如下图所示，
图2. RNN反向传播示意图
上面展示的是的情形，即输入和输出一样多，根据和，有以下多种网络形式。
图3. 多种类型的RNN模型
1.2 解决RNN的梯度消失 和深度神经网络一样，RNN同样存在梯度消失和梯度爆炸的问题（复合函数求导，若每一步梯度&gt;1则容易产生梯度爆炸，梯度&lt;1则容易产生梯度消失，参见https://zhuanlan.zhihu.com/p/68579467），导致更新网络参数无效或者震荡太大。梯度爆炸容易观察，但是梯度消失不易观察，针对RNN，专门有GRU和LSML两种模型解决。
GRU简单理解为，每个时刻引入一个记忆值,记忆值和激活值相等。根据门控（gate）决定是否用新得到的更新当前的c^{t},
图4. GRU示意图
更新公式如下：
（2.1）
（2.2）
（2.3）
（2.4）
（2.5）
与图4中只有一个门控不同的是，在计算的时候有另外一个相关性的门控。
LSMT和GRU不同的是，和并不相等，引入了其他的门控和,其中取代了式（2.4）中的，作为单独的遗忘门控；用以更新激活值，如下：
(3.1)
(3.2)
(3.3)
(3.4)
(3.5)
(3.6)
2. 实例 2.1 pytorch中语法 Torch.nn.RNN为内置的RNN网络。序列的激活值用表示，计算公式如下，
初始化一个RNN网络语法：
rnn=torch.nn.RNN(input_size, hidden_size, num_layers,nonlinearity, bias,batch_first,dropout, bidirectional) 参数
其中，一般用到的参数为input_size, hidden_size, num_layers,nonlinearity, bias,batch_first。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://zhuifengsn.github.io/posts/4dcd98323496289ee650c411813d32a0/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-05-07T12:01:23+08:00" />
<meta property="article:modified_time" content="2022-05-07T12:01:23+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="追风少年的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">追风少年的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">自然语言处理—RNN循环神经网络</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2>1. RNN基本介绍</h2> 
<p>(参见吴恩达深度学习—RNN篇<a class="link-info" href="https://www.bilibili.com/video/BV16r4y1Y7jv?p=152&amp;spm_id_from=pageDriver" rel="nofollow" title="https://www.bilibili.com/video/BV16r4y1Y7jv?p=152&amp;spm_id_from=pageDriver">https://www.bilibili.com/video/BV16r4y1Y7jv?p=152&amp;spm_id_from=pageDriver</a>)</p> 
<h3>1.1 基本介绍</h3> 
<p>RNN神经网络可以用来处理时间序列类型的数据，一段文字也可以看成一个时间序列。比如，</p> 
<p>The           cat       ate       many    food  that   was  delicious  was  full</p> 
<p><img alt="x^{(i)&lt;1&gt;}" class="mathcode" src="https://images2.imgbox.com/5e/31/zwGzLNoc_o.png">   <img alt="x^{(i)&lt;2&gt;}" class="mathcode" src="https://images2.imgbox.com/a9/e2/LkNd6Xjq_o.png">    <img alt="x^{(i)&lt;3&gt;}" class="mathcode" src="https://images2.imgbox.com/25/45/bMqLLx0I_o.png">          <img alt="\cdots" class="mathcode" src="https://images2.imgbox.com/83/fa/Gby4zMJr_o.png">          <img alt="x^{(i)&lt;t&gt;}" class="mathcode" src="https://images2.imgbox.com/1f/4d/4A68PCjw_o.png">         <img alt="\cdots" class="mathcode" src="https://images2.imgbox.com/bb/dd/zsgC616s_o.png">               <img alt="x^{(i)&lt;T_{x}&gt;}" class="mathcode" src="https://images2.imgbox.com/e5/cc/6WbnkPVO_o.png">                                 </p> 
<p>其中，上标<img alt="i" class="mathcode" src="https://images2.imgbox.com/7b/a7/8XDthh4y_o.png">代表第<img alt="(i)" class="mathcode" src="https://images2.imgbox.com/5b/5c/Lx9ZSsdt_o.png">个样本，<img alt="&lt;t&gt;" class="mathcode" src="https://images2.imgbox.com/18/25/iw6uDnaN_o.png">代表时间，下面将不再样本的标记<img alt="(i)" class="mathcode" src="https://images2.imgbox.com/b2/55/Zd4T0xt7_o.png">进行分析。一般的神经网络处理时间序列时存在两个问题：i) 输入维度不确定，且如果是one-hot编码，则向量维度高；ii) 一般神经网络不能捕捉序列之间的信息。因此，有循环神经网络来处理时间序列数据。</p> 
<p class="img-center"><img alt="" height="270" src="https://images2.imgbox.com/3c/9f/lxQHdY6f_o.png" width="494"></p> 
<p style="text-align:center;">图1. RNN神经网络模型结果</p> 
<p><strong>前向传播</strong></p> 
<p>在RNN中，每一时刻<img alt="t" class="mathcode" src="https://images2.imgbox.com/58/3b/hTuYLGkX_o.png">所使用的参数<img alt="W_{ax}" class="mathcode" src="https://images2.imgbox.com/f3/78/pJBGAoci_o.png">、<img alt="W_{ya}" class="mathcode" src="https://images2.imgbox.com/ef/ca/JPLDmjln_o.png">、<img alt="W_{aa}" class="mathcode" src="https://images2.imgbox.com/71/df/cGcW9XMO_o.png">以及<img alt="b_{ax}" class="mathcode" src="https://images2.imgbox.com/41/63/pARKKTBb_o.png">、<img alt="b_{aa}" class="mathcode" src="https://images2.imgbox.com/68/2f/Fzqy6DeH_o.png"> 、<img alt="b_{ya}" class="mathcode" src="https://images2.imgbox.com/fd/e6/NJC4J6lk_o.png">都是一致的。模型的forward-propagation如下：</p> 
<p style="text-align:center;"><img alt="a^{t} = tanh(W_{a}\begin{bmatrix} a^{t-1}\\ x^{t} \end{bmatrix} + b_{a})" class="mathcode" src="https://images2.imgbox.com/8d/ad/i2MZBu6Y_o.png"></p> 
<p style="text-align:center;"><img alt="y^{t} = tanh(W_{y} a^{t}+ b_{y})" class="mathcode" src="https://images2.imgbox.com/31/7c/j9FRoz4B_o.png">  （1）</p> 
<p>其中，<img alt="W_{a}=\begin{bmatrix} W_{aa} &amp; W_{ax} \end{bmatrix}" class="mathcode" src="https://images2.imgbox.com/2b/a2/ILqkY4Nv_o.png">。</p> 
<p><strong>反向传播</strong></p> 
<p>每一个时刻，都有损失函数：</p> 
<p style="text-align:center;"><img alt="L^{&lt;t&gt;}(y^{t}, \hat{y}^{t}) = -y^{t}log(\hat{y}^{t})-(1-y^{t})log(1-\hat{y}^{t})" class="mathcode" src="https://images2.imgbox.com/35/2b/1Rw7EVaQ_o.png"></p> 
<p>总的损失为<img alt="L = \sum_{t} L^{&lt;t&gt;}" class="mathcode" src="https://images2.imgbox.com/49/45/X1KXK3qY_o.png">。其back-propagation如下图所示，</p> 
<p class="img-center"><img alt="" height="373" src="https://images2.imgbox.com/fa/47/9jdEuzsL_o.png" width="529"></p> 
<p style="text-align:center;">图2. RNN反向传播示意图</p> 
<p>上面展示的是<img alt="T_{x} = T_{y}" class="mathcode" src="https://images2.imgbox.com/14/b4/V8CAr5hJ_o.png">的情形，即输入和输出一样多，根据<img alt="T_{x}" class="mathcode" src="https://images2.imgbox.com/34/5a/riszLMzD_o.png">和<img alt="T_{y}" class="mathcode" src="https://images2.imgbox.com/86/c0/NzST4BxQ_o.png">，有以下多种网络形式。</p> 
<p class="img-center"><img alt="" height="264" src="https://images2.imgbox.com/1f/ac/9IA4pM8H_o.png" width="466"></p> 
<p style="text-align:center;"> 图3. 多种类型的RNN模型</p> 
<h3> 1.2 解决RNN的梯度消失</h3> 
<p>和深度神经网络一样，RNN同样存在梯度消失和梯度爆炸的问题（复合函数求导，若每一步梯度&gt;1则容易产生梯度爆炸，梯度&lt;1则容易产生梯度消失，参见<a class="link-info" href="https://zhuanlan.zhihu.com/p/68579467" rel="nofollow" title="https://zhuanlan.zhihu.com/p/68579467">https://zhuanlan.zhihu.com/p/68579467</a>），导致更新网络参数无效或者震荡太大。梯度爆炸容易观察，但是梯度消失不易观察，针对RNN，专门有GRU和LSML两种模型解决。</p> 
<p>GRU简单理解为，每个时刻<img alt="t" class="mathcode" src="https://images2.imgbox.com/16/be/i6uXFBrt_o.png">引入一个记忆值<img alt="c^{t}" class="mathcode" src="https://images2.imgbox.com/4f/a7/JqGZVy5S_o.png">,记忆值<img alt="c^{t}" class="mathcode" src="https://images2.imgbox.com/3c/85/UuCka75K_o.png">和激活值<img alt="a^{t}" class="mathcode" src="https://images2.imgbox.com/f1/89/O3lNRffO_o.png">相等。根据门控<img alt="\Gamma _{u}" class="mathcode" src="https://images2.imgbox.com/6f/a6/Xs8oJRdc_o.png">（gate）决定是否用新得到的<img alt="\tilde{c}^{&lt;t&gt;}" class="mathcode" src="https://images2.imgbox.com/aa/74/I5EcVye5_o.png">更新当前的c^{t},</p> 
<p class="img-center"><img alt="" height="454" src="https://images2.imgbox.com/a5/37/v07JqtDL_o.png" width="689"></p> 
<p style="text-align:center;">图4. GRU示意图</p> 
<p> 更新公式如下：</p> 
<p style="text-align:center;"><img alt="\tilde{c}^{&lt;t&gt;} = tanh(W_{c}\begin{bmatrix} \Gamma_{r} c^{&lt;t-1&gt;} \\ x^{&lt;t&gt;} \end{bmatrix}+ b_{c})" class="mathcode" src="https://images2.imgbox.com/86/47/HK8Bhr9v_o.png">（2.1）</p> 
<p style="text-align:center;"><img alt="\Gamma_{u} = tanh(W_{u}\begin{bmatrix} c^{&lt;t-1&gt;} \\ x^{&lt;t&gt;} \end{bmatrix}+ b_{u})" class="mathcode" src="https://images2.imgbox.com/f0/4c/wICf5xqI_o.png"> （2.2）</p> 
<p style="text-align:center;"> <img alt="\Gamma_{r} = tanh(W_{r}\begin{bmatrix} c^{&lt;t-1&gt;} \\ x^{&lt;t&gt;} \end{bmatrix}+ b_{r})" class="mathcode" src="https://images2.imgbox.com/ba/74/DPVxbYhj_o.png"> （2.3）</p> 
<p style="text-align:center;"><img alt="c^{&lt;t&gt;} = \Gamma_{u} \tilde{c}^{&lt;t&gt;} + (1- \Gamma_{u})c^{&lt;t-1&gt;}" class="mathcode" src="https://images2.imgbox.com/8f/aa/nL3Z56gs_o.png">（2.4）</p> 
<p style="text-align:center;"><img alt="a^{&lt;t&gt;} = c^{&lt;t&gt;}" class="mathcode" src="https://images2.imgbox.com/6e/d8/GQTJyDmP_o.png">（2.5）</p> 
<p>与图4中只有一个门控<img alt="\Gamma_{u}" class="mathcode" src="https://images2.imgbox.com/a3/ce/SN1zbWoZ_o.png">不同的是，在计算<img alt="\tilde{c}^{&lt;t&gt;}" class="mathcode" src="https://images2.imgbox.com/d7/1f/0o4s31MB_o.png">的时候有另外一个相关性的门控<img alt="\Gamma_r" class="mathcode" src="https://images2.imgbox.com/ad/c5/nanmxYV6_o.png">。</p> 
<p>LSMT和GRU不同的是，<img alt="a^{&lt;t&gt;}" class="mathcode" src="https://images2.imgbox.com/7c/6a/VY8rgpxe_o.png">和<img alt="c^{&lt;t&gt;}" class="mathcode" src="https://images2.imgbox.com/7e/d2/El4p8o3Q_o.png">并不相等，引入了其他的门控<img alt="\Gamma_{f}" class="mathcode" src="https://images2.imgbox.com/fe/70/xDYeBEJA_o.png">和<img alt="\Gamma_{o}" class="mathcode" src="https://images2.imgbox.com/ef/d7/yXa7xklt_o.png">,其中<img alt="\Gamma_{f}" class="mathcode" src="https://images2.imgbox.com/e6/be/z1iOxwpq_o.png">取代了式（2.4）中的<img alt="(1- \Gamma_{u})" class="mathcode" src="https://images2.imgbox.com/28/88/gFbQp75z_o.png">，作为单独的遗忘门控；<img alt="\Gamma_{o}" class="mathcode" src="https://images2.imgbox.com/86/2d/I0uOgNFr_o.png">用以更新激活值<img alt="a^{&lt;t&gt;}" class="mathcode" src="https://images2.imgbox.com/df/97/ddBJ5dzu_o.png">，如下：</p> 
<p style="text-align:center;"><img alt="\tilde{c}^{&lt;t&gt;} = tanh(W_{c}\begin{bmatrix} c^{&lt;t-1&gt;} \\ x^{&lt;t&gt;} \end{bmatrix}+ b_{c})" class="mathcode" src="https://images2.imgbox.com/d4/5c/eORNmctM_o.png"> (3.1)</p> 
<p style="text-align:center;"><img alt="\Gamma_{u} = tanh(W_{u}\begin{bmatrix} c^{&lt;t-1&gt;} \\ x^{&lt;t&gt;} \end{bmatrix}+ b_{u})" class="mathcode" src="https://images2.imgbox.com/42/0f/flQ8nR18_o.png">(3.2)</p> 
<p style="text-align:center;"><img alt="\Gamma_{f} = tanh(W_{f}\begin{bmatrix} c^{&lt;t-1&gt;} \\ x^{&lt;t&gt;} \end{bmatrix}+ b_{f})" class="mathcode" src="https://images2.imgbox.com/5b/2c/p0p5tej8_o.png">(3.3)</p> 
<p style="text-align:center;"><img alt="\Gamma_{o} = tanh(W_{o}\begin{bmatrix} c^{&lt;t-1&gt;} \\ x^{&lt;t&gt;} \end{bmatrix}+ b_{o})" class="mathcode" src="https://images2.imgbox.com/ab/c0/V1036mwR_o.png">(3.4)</p> 
<p style="text-align:center;"><img alt="c^{&lt;t&gt;} = \Gamma_{u} \tilde{c}^{&lt;t&gt;} + \Gamma_{f}c^{&lt;t-1&gt;}" class="mathcode" src="https://images2.imgbox.com/93/87/J0EJ8QeI_o.png">(3.5)</p> 
<p style="text-align:center;"><img alt="a^{&lt;t&gt;} = \Gamma_{o}c^{&lt;t&gt;}" class="mathcode" src="https://images2.imgbox.com/a3/32/QQAJNyHq_o.png">(3.6)</p> 
<h2>2. 实例</h2> 
<h3>2.1 pytorch中语法</h3> 
<p>Torch.nn.RNN为内置的RNN网络。序列<img alt="i" class="mathcode" src="https://images2.imgbox.com/72/01/HSEYZBy6_o.png">的激活值用<img alt="h_{i}" class="mathcode" src="https://images2.imgbox.com/5f/33/zLf7B5JP_o.png">表示，计算公式如下，</p> 
<p style="text-align:center;"><img alt="h^{t} = tanh(W_{a}\begin{bmatrix} h^{t-1}\\ x^{t} \end{bmatrix} + b_{h})" class="mathcode" src="https://images2.imgbox.com/ed/be/XemG8IVE_o.png"></p> 
<p>初始化一个RNN网络语法：</p> 
<pre><code class="language-python">rnn=torch.nn.RNN(input_size, hidden_size, num_layers,nonlinearity, bias,batch_first,dropout, bidirectional)</code></pre> 
<p><strong>参数</strong></p> 
<p>其中，一般用到的参数为input_size, hidden_size, num_layers,nonlinearity, bias,batch_first。</p> 
<p>input_size：输入序列每个字节的维度<br> hidden_size：隐含层中的激活值的维度<br> num_layers：隐含层的层数</p> 
<p>nonlinearity：默认：激活函数为tanh，也可以设置为relu等</p> 
<p>bias：是否有偏重<img alt="b_{h}" class="mathcode" src="https://images2.imgbox.com/0f/04/KLfZgdoW_o.png">，默认为true</p> 
<p><strong>网络输入</strong></p> 
<p>Input—X：[seq_len, batch_size, input_size]<br> seq_len：输入的句子/序列的字节长度<br> batch_size：样本量<br> input_size：字节维度</p> 
<p>Input—h0: [num_layers, batch_size, hidden_size]<br> num_layers：多少层隐含层，多少层的激活值<br> batch_size：样本量<br> hidden_size：激活值的维度</p> 
<p>当网络的设置中batch_first为True时，输入为Input—X：[seq_len, batch_size, input_size]，Input—h0: [num_layers, batch_size, hidden_size]</p> 
<p><strong>网络输出</strong></p> 
<p>out-out（n-n类型RNN）<br> [seq_len,batch_size,hidden_size]<br> out-h（最后一个字节对应的激活值）<br> [seq_len,batch_size,hiden_size]</p> 
<p>参加pytorch官网<a class="link-info" href="https://pytorch.org/docs/stable/generated/torch.nn.RNN.html#torch.nn.RNN" rel="nofollow" title="https://pytorch.org/docs/stable/generated/torch.nn.RNN.html#torch.nn.RNN">https://pytorch.org/docs/stable/generated/torch.nn.RNN.html#torch.nn.RNN</a>、<a href="https://blog.csdn.net/weixin_45727931/article/details/114369073" title="（超详细！！）Pytorch循环神经网络（RNN）快速入门与实战_Hello3q3q的博客-CSDN博客_rnn循环神经网络">（超详细！！）Pytorch循环神经网络（RNN）快速入门与实战_Hello3q3q的博客-CSDN博客_rnn循环神经网络</a></p> 
<p><strong>示例</strong></p> 
<p>设计一个含有2层隐含层的RNN，输入输出示意图如下，</p> 
<p><img alt="" height="623" src="https://images2.imgbox.com/23/a3/3Xfr10Eb_o.png" width="1022"></p> 
<p style="text-align:center;"> 图5. 2层RNN输入批量数据输出示意图</p> 
<p>在该例中，输入样本量<img alt="batch_size = 2" class="mathcode" src="https://images2.imgbox.com/40/ee/JE0UN11w_o.png">的<img alt="X" class="mathcode" src="https://images2.imgbox.com/e0/ce/uFijCqLy_o.png">，序列长度<img alt="seq_len = 3" class="mathcode" src="https://images2.imgbox.com/a1/22/zWMLJhB8_o.png">，每个字节的维度为<img alt="input_size = 10" class="mathcode" src="https://images2.imgbox.com/b3/4e/srSbGJLa_o.png">。设计隐含层2层，激活值的维度为<img alt="hidden_size = 3" class="mathcode" src="https://images2.imgbox.com/18/4a/ZM79Hmjd_o.png">。代码示例如下：</p> 
<pre><code class="language-python">'''
RNN
input_size：输入序列每个字节的维度
hidden_size：隐含层中的激活值的维度
num_layers：隐含层的层数

Input—X
seq_len：输入的句子/序列的字节长度
batch_size：样本量
input_size：字节维度

Input—h(初始化的激活层)
num_layers：多少层隐含层，多少层的激活值
batch_size：样本量
hidden_size：激活值的维度

out-out
seq_len,batch_size,hidden_size
out-h
seq_len,batch_size,hiden_size
'''
input_size = 10
hidden_size = 3
num_layers = 2
output_size = 2
rnn = nn.RNN(input_size=input_size,hidden_size=hidden_size,
             num_layers=num_layers, batch_first=True)

seq_len = 3        
batch_size = 2      
x = torch.randn(batch_size,seq_len,input_size)        # 输入数据
h0 = torch.zeros(batch_size,num_layers,hidden_size)   # 输入数据

out, h = rnn(x, h0)  # 输出数据
linear = nn.Linear(hidden_size, output_size)

print("out.shape:",out.shape)
print("h.shape:",h.shape)
print("out",out)
out = linear(out)
print(out)</code></pre> 
<p>其中，linear层将输出<img alt="out" class="mathcode" src="https://images2.imgbox.com/ba/44/rjvNpwbW_o.png">由<img alt="hidden\_size" class="mathcode" src="https://images2.imgbox.com/6d/85/VIZp3m1E_o.png">的维度转变为我们想要的维度<img alt="output\_size" class="mathcode" src="https://images2.imgbox.com/d7/9e/bOaXrys3_o.png">。</p> 
<p>结果如下，</p> 
<pre><code>out.shape: torch.Size([2, 3, 3])  # [batch_size, seq_len, hidden_size]
h.shape: torch.Size([2, 2, 3])    # [batch_size, Layers_num, hidden_size]
out tensor([[[-0.1594, -0.4284, -0.0468],    # 样本1字节1
         [ 0.0161, -0.5916, -0.1567],        # 样本1字节2
         [-0.0266, -0.6426, -0.1186]],       # 样本1字节3

        [[ 0.1776,  0.2767,  0.3266],        # 样本2字节1
         [ 0.0168,  0.1360,  0.0264],        # 样本2字节2
         [ 0.1708, -0.0948,  0.2696]]], grad_fn=&lt;TransposeBackward1&gt;)     # 样本2字节3
tensor([[[-0.2269, -0.0797],    # 样本1字节1
         [-0.1802,  0.0406],    # 样本1字节2
         [-0.1426,  0.0049]],   # 样本1字节1

        [[-0.7844, -0.0692],    # 样本2字节1
         [-0.6108, -0.0201],    # 样本2字节2
         [-0.5674, -0.0560]]], grad_fn=&lt;AddBackward0&gt;)    # 样本2字节1</code></pre> 
<p>（注：这里，为方便理解，RNN的参数batch_first=True，初始化x和h0的时候使用了对应的顺序，但是规范的方法应该是使用<a href="https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence" rel="nofollow" title="torch.nn.utils.rnn.PackedSequence">torch.nn.utils.rnn.PackedSequence</a>）</p> 
<h3>2.2 网络训练</h3> 
<p>构建RNN模型，训练网络</p> 
<pre><code class="language-python">class RNN_model(torch.nn.Module):
    def __init__(self, input_size, hidden_size,num_layers, output_size):
        super(RNN_model,self).__init__()
        self.rnn = nn.RNN(input_size = input_size,
                      hidden_size = hidden_size,
                      num_layers = num_layers,
                      batch_first=False)                     
        self.linear = nn.Linear(hidden_size, output_size)
    def forward(self, x, h0):
        out, h = self.rnn(x, h0)
        out = self.linear(out)
        return out

input_size = 10
hidden_size = 3
num_layers = 2
output_size = 2
model = RNN_model(input_size, hidden_size, num_layers, output_size)
model = model.to(device)</code></pre> 
<p>使用GRU训练，基本一致，将nn.RNN替换为nn.GRU</p> 
<pre><code class="language-python">class RNN_model(torch.nn.Module):
    def __init__(self, input_size, hidden_size,num_layers, output_size):
        super(RNN_model,self).__init__()
        self.gru = nn.GRU(input_size = input_size,
                         hidden_size = hidden_size,
                         num_layers = num_layers,
                         batch_first=False)     
        self.linear = nn.Linear(hidden_size, output_size)
    def forward(self, x, h0):
        out, h = self.gru(x, h0)
        out = self.linear(out)
        return out

input_size = 10
hidden_size = 3
num_layers = 2
output_size = 2
model = RNN_model(input_size, hidden_size, num_layers, output_size)
model = model.to(device)</code></pre> 
<p>训练过程如下，</p> 
<pre><code class="language-python">seq_len = 3        # 句子长度
batch_size = 2      
x = torch.randn(seq_len,batch_size,input_size)        # 输入数据
h0 = torch.zeros(num_layers,batch_size,hidden_size)   # 输入数据
target = torch.randn(seq_len,batch_size,output_size)  # 输出数据

board = SummaryWriter('/kaggle/working/ML_RNN/logs')
loss_function = nn.MSELoss()
opt = torch.optim.Adam(model.parameters(), lr=0.003, weight_decay=1e-3)
Epochs = 100
for epoch in range(Epochs):
    pred = model(x, h0)
    loss = loss_function(pred, target)
    #一般下面三行指令是放一起的
    opt.zero_grad()
    loss.backward()
    opt.step() 
    print('epoch=',epoch,' train_loss=',loss.item())
    board.add_scalar("Train_loss", loss.item(), epoch)
board.close()
</code></pre> 
<p>误差图如下， </p> 
<p class="img-center"><img alt="" height="356" src="https://images2.imgbox.com/79/27/8oZ2jnrb_o.png" width="533"></p> 
<p style="text-align:center;">图6. 误差图</p> 
<p>从误差图可以看到，训练到100步的时候，还未收敛。 </p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/af4855acd8d3f387423bdfb75f0f94bf/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">数据结构——栈</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/cd867a9bc4f0aedbc9d4472701ec290a/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">新闻个性化推荐综述</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 追风少年的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>