<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>深度学习——早停法（Early Stopping） - 追风少年的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="深度学习——早停法（Early Stopping）" />
<meta property="og:description" content="学习链接：https://www.jianshu.com/p/9ab695d91459
https://www.datalearner.com/blog/1051537860479157
目的： 为了获得性能良好的神经网络，网络定型过程中需要进行许多关于所用设置（超参数）的决策。
超参数之一是定型周期（epoch）的数量：亦即应当完整遍历数据集多少次（一次为一个epoch）？如果epoch数量太少，网络有可能发生欠拟合（即对于定型数据的学习不够充分）；如果epoch数量太多，则有可能发生过拟合（即网络对定型数据中的“噪声”而非信号拟合）。
早停法旨在解决epoch数量需要手动设置的问题。它也可以被视为一种能够避免网络发生过拟合的正则化方法（与L1/L2权重衰减和丢弃法类似）。
根本原因就是因为继续训练会导致测试集上的准确率下降。
那继续训练导致测试准确率下降的原因猜测可能是1. 过拟合 2. 学习率过大导致不收敛
原理 1.将数据分为训练集和验证集
2.每个epoch结束后（或每N个epoch后）： 在验证集上获取测试结果，随着epoch的增加，如果在验证集上发现测试误差上升，则停止训练；
3.将停止之后的权重作为网络的最终参数。
这种做法很符合直观感受，因为精度都不再提高了，在继续训练也是无益的，只会提高训练的时间。
那么该做法的一个重点便是怎样才认为验证集精度不再提高了呢？并不是说验证集精度一降下来便认为不再提高了，因为可能经过这个Epoch后，精度降低了，但是随后的Epoch又让精度又上去了，所以不能根据一两次的连续降低就判断不再提高。
一般的做法是，在训练的过程中，记录到目前为止最好的验证集精度，当连续10次Epoch（或者更多次）没达到最佳精度时，则可以认为精度不再提高了。
直观感受：
最优模型是在垂直虚线的时间点保存下来的模型，即处理测试集时准确率最高的模型。
为什么能够减小拟合：
当还未在神经网络运行太多迭代过程的时候，w参数接近于0，因为随机初始化w值的时候，它的值是较小的随机值。当你开始迭代过程，w的值会变得越来越大。到后面时，w的值已经变得十分大了。所以early stopping要做的就是在中间点停止迭代过程。我们将会得到一个中等大小的w参数，会得到与L2正则化相似的结果，选择了w参数较小的神经网络。
Early Stopping的缺点：
没有采取不同的方式来解决优化损失函数和降低方差这两个问题，而是用一种方法同时解决两个问题 ，结果就是要考虑的东西变得更复杂。之所以不能独立地处理，因为如果你停止了优化代价函数，你可能会发现代价函数的值不够小，同时你又不希望过拟合
作者：zzkdev
链接：https://www.jianshu.com/p/9ab695d91459
来源：简书
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。
作者：zzkdev
链接：https://www.jianshu.com/p/9ab695d91459
来源：简书
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://zhuifengsn.github.io/posts/6471d7411a7510d1807c65ff13765f77/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-04-06T09:55:32+08:00" />
<meta property="article:modified_time" content="2021-04-06T09:55:32+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="追风少年的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">追风少年的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">深度学习——早停法（Early Stopping）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>学习链接：<a href="https://www.jianshu.com/p/9ab695d91459" rel="nofollow">https://www.jianshu.com/p/9ab695d91459</a><br> <a href="https://www.datalearner.com/blog/1051537860479157" rel="nofollow">https://www.datalearner.com/blog/1051537860479157</a></p> 
<h3><a id="_3"></a>目的：</h3> 
<p>为了获得性能良好的神经网络，网络定型过程中需要进行许多关于所用设置（超参数）的决策。<br> 超参数之一是定型周期（epoch）的数量：亦即应当完整遍历数据集多少次（一次为一个epoch）？如果epoch数量太少，网络有可能发生欠拟合（即对于定型数据的学习不够充分）；如果epoch数量太多，则有可能发生过拟合（即网络对定型数据中的“噪声”而非信号拟合）。</p> 
<p>早停法旨在解决epoch数量需要手动设置的问题。它也可以被视为一种能够避免网络发生过拟合的正则化方法（与L1/L2权重衰减和丢弃法类似）。</p> 
<p>根本原因就是因为继续训练会导致测试集上的准确率下降。<br> 那继续训练导致测试准确率下降的原因猜测可能是1. 过拟合 2. 学习率过大导致不收敛</p> 
<h3><a id="_13"></a>原理</h3> 
<p>1.将数据分为训练集和验证集<br> 2.每个epoch结束后（或每N个epoch后）： 在验证集上获取测试结果，<strong>随着epoch的增加，如果在验证集上发现测试误差上升，则停止训练；</strong><br> 3.<strong>将停止之后的权重作为网络的最终参数。</strong></p> 
<p>这种做法很符合直观感受，因为精度都不再提高了，在继续训练也是无益的，只会提高训练的时间。<br> 那么该做法的一个重点便是怎样才认为验证集精度不再提高了呢？并不是说验证集精度一降下来便认为不再提高了，因为可能经过这个Epoch后，精度降低了，但是随后的Epoch又让精度又上去了，所以不能根据一两次的连续降低就判断不再提高。<br> 一般的做法是，在训练的过程中，记录到目前为止最好的验证集精度，当连续10次Epoch（或者更多次）没达到最佳精度时，则可以认为精度不再提高了。<br> 直观感受：<br> <img src="https://images2.imgbox.com/05/eb/mJWuzDXi_o.png" alt="在这里插入图片描述"><br> <strong>最优模型是在垂直虚线的时间点保存下来的模型，即处理测试集时准确率最高的模型。</strong></p> 
<p>为什么能够减小拟合：<br> 当还未在神经网络运行太多迭代过程的时候，w参数接近于0，因为随机初始化w值的时候，它的值是较小的随机值。当你开始迭代过程，w的值会变得越来越大。到后面时，w的值已经变得十分大了。所以early stopping要做的就是在中间点停止迭代过程。我们将会得到一个中等大小的w参数，会得到与L2正则化相似的结果，选择了w参数较小的神经网络。</p> 
<p>Early Stopping的缺点：<br> 没有采取不同的方式来解决优化损失函数和降低方差这两个问题，而是用一种方法同时解决两个问题 ，结果就是要考虑的东西变得更复杂。之所以不能独立地处理，因为如果你停止了优化代价函数，你可能会发现代价函数的值不够小，同时你又不希望过拟合</p> 
<p>作者：zzkdev<br> 链接：https://www.jianshu.com/p/9ab695d91459<br> 来源：简书<br> 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p> 
<p>作者：zzkdev<br> 链接：https://www.jianshu.com/p/9ab695d91459<br> 来源：简书<br> 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/d034d4e0382ce00eb8902960b1457d35/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">用户的埋点和用户行为分析实战（埋点数据分析）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/ed9f4b8f879ddbb59fda1057ea3a2810/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">import os</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 追风少年的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>