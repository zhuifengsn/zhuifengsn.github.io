<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>神经网络中的epoch、batch、batch_size、iteration的理解 - 追风少年的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="神经网络中的epoch、batch、batch_size、iteration的理解" />
<meta property="og:description" content="神经网络中的epoch、batch、batch_size、iteration的理解
下面说说这三个区别：
（1）batchsize：批大小。在深度学习中，一般采用SGD训练，即每次训练在训练集中取batchsize个样本训练；
（2）iteration：1个iteration等于使用batchsize个样本训练一次；
（3）epoch：1个epoch等于使用训练集中的全部样本训练一次。
举个例子，训练集有1000个样本，batchsize=10，那么：
训练完整个样本集需要：100次iteration，1次epoch。
1、epoch 当一个完整的数据集通过神经网络一次并且返回一次的过程称为一个epoch。
一个epoch=所有训练样本的一个正向传递和一个反向传递。
然而，当一个epoch对于计算机太过庞大时，就需要把它分成多个小块（batch）。
2、iteration：中文翻译为迭代 iteration是重复反馈的动作，神经网络中我们希望通过迭代进行多次的训练以达到所需的目标或结果。iteration=step
每一次迭代得到的结果都会被作为下一次迭代的初始值。
一个迭代=一个正向通过&#43;一个反向通过。
3、batch 在不能将数据一次性通过神经网络的适合，就需要将数据集分成几个batch。
4、batch_size 直观的理解：一个batch中的样本总数（一次训练所选取的样本数）。
batch_size的大小影响模型的优化程度和速度，同时其直接影响到GPU内存的使用情况。
假如GPU显存不大，该数值最好设置小一点。
5、为什么要提出Batch Size？ 在没有使用Batch Size之前，这意味着网络在训练时，是一次把所有的数据（整个数据库）输入网络中，然后计算它们的梯度进行反向传播，由于在计算梯度时使用了整个数据库，所以计算得到的梯度方向更为准确。
但在这情况下，计算得到不同梯度值差别巨大，难以使用一个全局的学习率，所以这时一般使用Rprop这种基于梯度符号的训练算法，单独进行梯度更新。
在小样本数的数据库中，不使用Batch Size是可行的，而且效果也很好。但是一旦是大型的数据库，一次性把所有数据输进网络，肯定会引起内存的爆炸。所以就提出Batch Size的概念。
6、Batch Size设置合适时的优点 1、通过并行化提高内存的利用率。就是尽量让GPU满载运行，提高训练速度。
2、单个epoch的迭代次数减少了，参数的调整也慢了，假如要达到相同的识别精度，需要更多的epoch。
3、适当Batch Size使得梯度下降方向更加准确。
7、Batch Size从小到大的变化对网络影响 1、没有Batch Size，梯度准确，只适用于小样本数据库
2、Batch Size=1，梯度变来变去，非常不准确，网络很难收敛
3、Batch Size增大，梯度变准确
4、Batch Size增大，梯度已经非常准确，再增加Batch Size也没有用
注意：Batch Size增大了，要到达相同的准确度，必须要增大epoch。
GD（Gradient Descent）：就是没有利用Batch Size，用基于整个数据库得到梯度，梯度准确，但数据量大时，计算非常耗时，同时神经网络常是非凸的，网络最终可能收敛到初始点附近的局部最优点。
SGD（Stochastic Gradient Descent）：就是Batch Size=1，每次计算一个样本，梯度不准确，所以学习率要降低。
mini-batch SGD：就是选着合适Batch Size的SGD算法，mini-batch利用噪声梯度，一定程度上缓解了GD算法直接掉进初始点附近的局部最优值。同时梯度准确了，学习率要加大。
对于mini-batch SGD:
为什么需要有Batch_Size： batchsize的正确选择是为了在内存效率和内存容量之间寻找最佳平衡。
Batch_Size的取值：
全批次（蓝色） 如果数据集比较小，我们就采用全数据集。全数据集确定的方向能够更好的代表样本总体，从而更准确的朝向极值所在的方向。
注：对于大的数据集，我们不能使用全批次，因为会得到更差的结果。
迷你批次（绿色） 选择一个适中的Batch_Size值。就是说我们选定一个batch的大小后，将会以batch的大小将数据输入深度学习的网络中，然后计算这个batch的所有样本的平均损失，即代价函数是所有样本的平均。
随机（Batch_Size等于1的情况）（红色） 每次修正方向以各自样本的梯度方向修正，横冲直撞各自为政，难以达到收敛。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://zhuifengsn.github.io/posts/70325a4237218ec6ec7eec0a041fd1e2/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-04-10T19:30:14+08:00" />
<meta property="article:modified_time" content="2023-04-10T19:30:14+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="追风少年的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">追风少年的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">神经网络中的epoch、batch、batch_size、iteration的理解</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p style="text-align:justify;">神经网络中的epoch、batch、batch_size、iteration的理解</p> 
<p style="text-align:justify;">下面说说这<strong>三个区别</strong>：</p> 
<p style="text-align:justify;">（1）batchsize：批大小。在深度学习中，一般采用SGD训练，即每次训练在训练集中取batchsize个样本训练；<br> （2）iteration：1个iteration等于<strong>使用batchsize个样本</strong>训练一次；<br> （3）epoch：1个epoch等于<strong>使用训练集中的全部样本</strong>训练一次。</p> 
<p style="text-align:justify;">举个例子，训练集有1000个样本，batchsize=10，那么：<br> 训练完整个样本集需要：100次iteration，1次epoch。</p> 
<h4 style="text-align:justify;">1、epoch</h4> 
<p style="text-align:justify;">当一个<span style="color:#fe2c24;">完整的数据集</span>通过神经网络一次并且返回一次的过程称为一个epoch。</p> 
<p style="text-align:justify;"><strong>一个epoch=所有训练样本的一个正向传递和一个反向传递。</strong></p> 
<p style="text-align:justify;">然而，当一个epoch对于计算机太过庞大时，就需要把它分成多个小块（batch）。</p> 
<h4 style="text-align:justify;">2、iteration：中文翻译为迭代</h4> 
<p style="text-align:justify;">iteration是重复反馈的动作，神经网络中我们希望通过迭代进行多次的训练以达到所需的目标或结果。iteration=step</p> 
<p style="text-align:justify;"><strong>每一次迭代得到的结果都会被作为下一次迭代的初始值。</strong></p> 
<p style="text-align:justify;"><strong>一个迭代=一个正向通过+一个反向通过。</strong></p> 
<h4 style="text-align:justify;">3、batch</h4> 
<p style="text-align:justify;">在不能将数据一次性通过神经网络的适合，就需要将数据集分成几个batch。</p> 
<h4 style="text-align:justify;">4、batch_size</h4> 
<p style="text-align:justify;">直观的理解：<span style="color:#fe2c24;">一个batch中的样本总数（一次训练所选取的样本数）</span>。</p> 
<p style="text-align:justify;">batch_size的大小影响模型的优化程度和速度，同时其直接影响到GPU内存的使用情况。</p> 
<p style="text-align:justify;">假如GPU显存不大，该数值最好设置小一点。</p> 
<h4 style="text-align:justify;">5、为什么要提出Batch Size？</h4> 
<p style="text-align:justify;">在没有使用Batch Size之前，这意味着网络在训练时，是一次把所有的数据（整个数据库）输入网络中，然后计算它们的梯度进行反向传播，由于在计算梯度时使用了整个数据库，所以计算得到的梯度方向更为准确。</p> 
<p style="text-align:justify;">但在这情况下，计算得到不同梯度值差别巨大，难以使用一个全局的学习率，所以这时一般使用Rprop这种基于梯度符号的训练算法，单独进行梯度更新。</p> 
<p style="text-align:justify;">在小样本数的数据库中，不使用Batch Size是可行的，而且效果也很好。但是一旦是大型的数据库，一次性把所有数据输进网络，肯定会引起<span style="color:#fe2c24;">内存的爆炸</span>。所以就提出Batch Size的概念。</p> 
<h4 style="text-align:justify;">6、Batch Size设置合适时的优点</h4> 
<p style="text-align:justify;">1、通过<span style="color:#fe2c24;">并行化</span>提高内存的利用率。就是尽量让GPU满载运行，提高训练速度。<br> 2、单个epoch的迭代次数减少了，参数的调整也慢了，假如要达到相同的识别精度，需要更多的epoch。<br> 3、适当Batch Size使得<span style="color:#fe2c24;">梯度下降方向更加准确</span>。</p> 
<h4 style="text-align:justify;">7、Batch Size从小到大的变化对网络影响</h4> 
<p style="text-align:justify;">1、没有Batch Size，梯度准确，只适用于<span style="color:#fe2c24;">小样本数据库</span><br> 2、Batch Size=1，梯度变来变去，非常不准确，网络很难收敛<br> 3、<span style="color:#fe2c24;">Batch Size增大，梯度变准确</span><br> 4、Batch Size增大，梯度已经非常准确，再增加Batch Size也没有用</p> 
<p style="text-align:justify;">注意：Batch Size增大了，要到达相同的准确度，必须要增大epoch。</p> 
<p style="text-align:justify;">GD（Gradient Descent）：就是<span style="color:#fe2c24;">没有利用Batch Size</span>，用基于整个数据库得到梯度，梯度准确，但数据量大时，计算非常<span style="color:#fe2c24;">耗时</span>，同时<strong>神经网络</strong>常是非凸的，网络最终可能收敛到初始点附近的局部最优点。</p> 
<p style="text-align:justify;">SGD（Stochastic Gradient Descent）：就是<span style="color:#fe2c24;">Batch Size=1</span>，每次计算一个样本，梯度不准确，所以学习率要降低。</p> 
<p style="text-align:justify;">mini-batch SGD：就是选着<span style="color:#fe2c24;">合适Batch Size</span>的SGD算法，mini-batch利用噪声梯度，一定程度上缓解了GD算法直接掉进初始点附近的局部最优值。同时梯度准确了，学习率要加大。<br> 对于mini-batch SGD:</p> 
<p style="text-align:justify;"><img alt="" height="129" src="https://images2.imgbox.com/c2/cf/e9zmMFPp_o.png" width="656"></p> 
<h4><strong>为什么需要有Batch_Size：</strong></h4> 
<p style="text-align:justify;">batchsize的正确选择是为了在内存效率和内存容量之间寻找最佳平衡。</p> 
<p style="text-align:justify;">Batch_Size的取值：</p> 
<p class="img-center"><img alt="" height="282" src="https://images2.imgbox.com/77/5f/dCSRHVHG_o.png" width="507"></p> 
<ul><li style="text-align:justify;"><strong>全批次（蓝色）</strong></li></ul> 
<p style="text-align:justify;">如果数据集比较小，我们就采用全数据集。全数据集确定的方向能够更好的代表样本总体，从而更准确的朝向极值所在的方向。</p> 
<p style="text-align:justify;">注：对于大的数据集，我们不能使用全批次，因为会得到更差的结果。</p> 
<ul><li style="text-align:justify;"><strong>迷你批次（绿色）</strong></li></ul> 
<p style="text-align:justify;">选择一个适中的Batch_Size值。就是说我们选定一个batch的大小后，将会以batch的大小将数据输入深度学习的网络中，然后计算这个batch的所有样本的平均损失，即代价函数是所有样本的平均。</p> 
<ul><li style="text-align:justify;"><strong>随机（Batch_Size等于1的情况）（红色）</strong></li></ul> 
<p style="text-align:justify;">每次修正方向以各自样本的梯度方向修正，横冲直撞各自为政，难以达到收敛。</p> 
<h4>8、为什么说Batch size的增大能使网络的梯度更准确？</h4> 
<p style="text-align:justify;"> 梯度的方差表示：</p> 
<p class="img-center"><img alt="" height="39" src="https://images2.imgbox.com/9b/15/FdM4r2ce_o.png" width="546"></p> 
<p>由于样本是随机选取的，满足独立同分布，所以所有样本具有相同的方差<img alt="" height="26" src="https://images2.imgbox.com/cb/ea/7iEcSxr7_o.png" width="103"><br> 所以上式可以简化成<img alt="" height="25" src="https://images2.imgbox.com/84/33/Sxz0A49r_o.png" width="167"><br> 可以看出当Batch size为m时，样本的方差减少m倍，梯度就更准确了。</p> 
<p>假如想要保持原来数据的梯度方差，可以增大学习率<strong>lr</strong><br><img alt="" height="33" src="https://images2.imgbox.com/67/f8/T489V6Xk_o.png" width="166">,只要<strong>lr</strong>取<img alt="" height="22" src="https://images2.imgbox.com/d8/85/0Brg6taX_o.png" width="36">，上式就变成<img alt="" height="28" src="https://images2.imgbox.com/c5/4c/G3mXy3nd_o.png" width="115"><br><strong>这也说明<span style="color:#fe2c24;">batch size设置较大时，一般学习率要增大</span>。但是lr的增大不是一开始就设置的很大，而是在训练过程中慢慢变大。</strong></p> 
<p style="text-align:justify;">一个具体例子分析：<br> 在分布式训练中，Batch size随着数据并行的workers增加而增大，假如baseline的Batch Size为<strong>B</strong>，而学习率为<strong>lr</strong>，训练epoch为<strong>N</strong>。假如保持baseline的<strong>lr</strong>，一般达不到很好的收敛速度和精度。<br> 原因：对于收敛速度，假如有<strong>K</strong>个workers，则每个批次为<strong>KB</strong>，因此一个epoch迭代的次数为baseline的<strong>1/k</strong>，而学习率<strong>lr</strong>不变，所以要达到与baseline相同的收敛情况，epoch要增大。而根据上面公式，epoch最大需要增大<strong>KN</strong>个epoch，但一般情况下不需要增大那么多。<br> 对于收敛精度，由于Batch size的使用使梯度更准确，噪声减少，所以更容易收敛。</p> 
<h4 style="text-align:justify;">适当的增加Batch_Size的优点：</h4> 
<p style="text-align:justify;">1.通过并行化提高内存利用率。</p> 
<p style="text-align:justify;">2.单次epoch的迭代次数减少，提高运行速度。</p> 
<p style="text-align:justify;">（单次epoch=(全部训练样本/batchsize)/iteration=1）</p> 
<p style="text-align:justify;">3.适当的增加Batch_Size,梯度下降方向准确度增加，训练震动的幅度减小。（看上图便可知晓）</p> 
<h4 style="text-align:justify;">经验总结：</h4> 
<p style="text-align:justify;"> 相对于正常数据集，如果Batch_Size过小，训练数据就会非常难收敛，从而导致underfitting。</p> 
<p style="text-align:justify;">增大Batch_Size,相对处理速度加快。</p> 
<p style="text-align:justify;">增大Batch_Size,所需内存容量增加（epoch的次数需要增加以达到最好的结果）</p> 
<p style="text-align:justify;">这里我们发现上面两个矛盾的问题，因为当epoch增加以后同样也会导致耗时增加从而速度下降。因此我们需要寻找最好的Batch_Size。</p> 
<p style="text-align:justify;"><span style="color:#38d8f0;"><strong>再次重申：Batch_Size的正确选择是为了在内存效率和内存容量之间寻找最佳平衡。</strong></span></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/257f3f74c0de068fff06c9af178e0401/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">RocketMQ Docker部署</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/095d30e7c2503875bc676a37dc36e2bc/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">如何将word翻译成中文？跟我学几招让你的文档秒变中文版</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 追风少年的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>