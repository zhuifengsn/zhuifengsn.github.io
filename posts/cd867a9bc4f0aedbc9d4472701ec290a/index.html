<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>新闻个性化推荐综述 - 追风少年的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="新闻个性化推荐综述" />
<meta property="og:description" content="MIND: A Large-scale Dataset for News Recommendation ACL 2020 0. 摘要 新闻推荐是实现个性化新闻服务的一项重要技术。与已经被广泛研究的产品推荐和电影推荐相比，新闻推荐的研究非常有限，主要是因为缺乏高质量的基准数据集。本文提出了一个名为MIND的新闻推荐大型数据集。MIND由微软新闻的用户点击日志构建而成，包含100万用户和16万多篇英文新闻文章，每篇文章都有丰富的标题、摘要、正文等文本内容。通过对几种最先进的新闻推荐方法的比较研究，证明了MIND为新闻推荐提供了一个良好的实验平台，这些方法最初是在不同的专有数据集上开发的。研究结果表明，新闻推荐的性能在很大程度上依赖于新闻内容理解和用户兴趣建模的质量。有效的文本表示方法和预先训练好的语言模型等自然语言处理技术可以有效地提高新闻推荐的性能。MIND数据集可以在MIND网站下载。
1. 引言 Google news、Microsoft news等在线新闻服务已经成为广大用户获取新闻信息的重要平台。每天都有大量的新闻生成并发布在网上，使得用户很难快速找到感兴趣的新闻。个性化新闻推荐可以帮助用户缓解信息过载，提高新闻阅读体验。
在传统的推荐系统中，通过协同过滤等方法来学习用户和项目表示。然而，新闻推荐面临着一些特殊的挑战。首先，新闻网站上的新闻文章更新非常快。新的新闻文章不断发布，现有的新闻文章会在短时间内过期。因此，新闻推荐中的冷启动问题非常严重。其次，新闻文章包含标题、正文等丰富的文本信息。简单地用id表示他们是不合适的，从他们的文本中理解他们的内容是很重要的。第三，对用户在新闻平台上发布的新闻文章没有明确的评分。因此在新闻推荐中，用户对新闻的兴趣通常是从用户的点击行为中隐含地推断出来的。
大规模和高质量的数据集可以显著促进某一领域的研究，例如ImageNet用于图像分类(Deng et al.， 2009)和机器阅读理解SQuAD(Rajpurkar et al.， 2016)，Amazon dataset用于产品推荐，MovieLens dataset用于电影推荐。然而，现有的关于新闻推荐的研究较少，很多都是在专有数据集上进行的(Okura et al.， 2017; Wang et al.， 2018;吴等，2019a)。
本文提出一个大规模的微软新闻数据集(MIND)用于新闻推荐研究，该数据集是从Microsoft News的用户行为日志中收集，包含了100万用户以及他们点击超过16万篇英语新闻的行为。在不同的专有数据集上实现了许多最新的新闻推荐方法，并比较它们在思维数据集上的表现，为新闻推荐研究提供基准。实验结果表明，利用NLP技术对新闻文章进行深入理解对新闻推荐具有重要意义。有效的文本表示方法和预先训练好的语言模型都有助于提高新闻推荐的性能。此外，适当地对用户兴趣进行建模也很有用。希望MIND可以作为新闻推荐的基准数据集，促进这一领域的研究。
2. 相关工作 2.1 新闻推荐 新闻推荐旨在从大量候选新闻中寻找用户感兴趣的新闻文章(Das et al.，2007)。新闻推荐中存在两个重要的问题，即如何表示文本内容丰富的新闻文章，以及如何从用户之前的行为中建模用户对新闻的兴趣(Okura et al.， 2017)。传统的新闻推荐方法通常依靠特征工程来代表新闻文章和用户兴趣。例如，Li等人(2010)使用url和类别来表示新闻文章，使用人口统计数据、地理信息和从他们在雅虎上的消费记录推断出的行为类别来表示用户。
近年来，一些基于深度学习的新闻推荐方法被提出，以端到端方式学习新闻文章的表示和用户兴趣(Okura et al., 2017; Wu et al., 2019a; An et al., 2019年)。例如，Okura等人(2017)使用去噪自动编码器模型表示来自新闻内容的新闻文章，使用GRU模型表示来自历史点击新闻文章的用户兴趣。他们在日本雅虎平台的实验显示，通过深度学习模型学习到的新闻和用户表示很有希望用于新闻推荐。Wang et al.(2018)提出通过结合从知识图谱中推导出的词向量和实体向量，利用CNN网络从新闻标题中学习知识感知的新闻表示。Wu et al. (2019a)提出了一种专注的多视角学习框架来表示标题、正文、类别等不同文本的新闻文章。他们使用了一个注意力模型，通过选择信息丰富的文章，从用户点击的新闻文章中推断出用户的兴趣。这些工作通常开发和验证的专有数据集，这是不可公开的，使它为其他研究人员验证这些方法和发展自己的方法困难。
新闻推荐与NLP有着丰富的内在关联。首先，新闻是一种常见的文本形式，文本建模技术CNN和Transformer可以很自然地被用来表示新闻文章(Wu et al., 2019a; Ge et al." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://zhuifengsn.github.io/posts/cd867a9bc4f0aedbc9d4472701ec290a/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-05-07T22:05:26+08:00" />
<meta property="article:modified_time" content="2022-05-07T22:05:26+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="追风少年的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">追风少年的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">新闻个性化推荐综述</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="MIND_A_Largescale_Dataset_for_News_Recommendation_1"></a>MIND: A Large-scale Dataset for News Recommendation</h2> 
<ul><li>ACL 2020</li></ul> 
<h3><a id="0__3"></a>0. 摘要</h3> 
<p>新闻推荐是实现个性化新闻服务的一项重要技术。与已经被广泛研究的产品推荐和电影推荐相比，<strong>新闻推荐的研究非常有限</strong>，主要是因为缺乏高质量的基准数据集。本文提出了一个名为MIND的新闻推荐大型数据集。MIND由微软新闻的用户点击日志构建而成，包含100万用户和16万多篇英文新闻文章，每篇文章都有丰富的标题、摘要、正文等文本内容。通过对几种最先进的新闻推荐方法的比较研究，证明了MIND为新闻推荐提供了一个良好的实验平台，这些方法最初是在不同的专有数据集上开发的。研究结果表明，新闻推荐的性能在很大程度上依赖于新闻内容理解和用户兴趣建模的质量。有效的文本表示方法和预先训练好的语言模型等自然语言处理技术可以有效地提高新闻推荐的性能。MIND数据集可以在<a href="https://msnews.github.io" rel="nofollow">MIND网站</a>下载。</p> 
<h3><a id="1__5"></a>1. 引言</h3> 
<p>Google news、Microsoft news等在线新闻服务已经成为广大用户获取新闻信息的重要平台。每天都有大量的新闻生成并发布在网上，使得用户很难快速找到感兴趣的新闻。个性化新闻推荐可以帮助用户缓解信息过载，提高新闻阅读体验。</p> 
<p>在传统的推荐系统中，通过协同过滤等方法来学习用户和项目表示。然而，新闻推荐面临着一些特殊的挑战。首先，新闻网站上的新闻文章更新非常快。新的新闻文章不断发布，现有的新闻文章会在短时间内过期。因此，<strong>新闻推荐中的冷启动问题非常严重</strong>。其次，<strong>新闻文章包含标题、正文等丰富的文本信息</strong>。简单地用id表示他们是不合适的，从他们的文本中理解他们的内容是很重要的。第三，<strong>对用户在新闻平台上发布的新闻文章没有明确的评分</strong>。因此在新闻推荐中，用户对新闻的兴趣通常是从用户的点击行为中隐含地推断出来的。</p> 
<p><strong>大规模和高质量的数据集可以显著促进某一领域的研究</strong>，例如<strong>ImageNet</strong>用于图像分类(Deng et al.， 2009)和机器阅读理解<strong>SQuAD</strong>(Rajpurkar et al.， 2016)，<strong>Amazon dataset</strong>用于产品推荐，<strong>MovieLens dataset</strong>用于电影推荐。然而，现有的关于新闻推荐的研究较少，很多都是在专有数据集上进行的(Okura et al.， 2017; Wang et al.， 2018;吴等，2019a)。</p> 
<p>本文提出一个大规模的微软新闻数据集(MIND)用于新闻推荐研究，该数据集是从Microsoft News的用户行为日志中收集，包含了100万用户以及他们点击超过16万篇英语新闻的行为。在不同的专有数据集上实现了许多最新的新闻推荐方法，并比较它们在思维数据集上的表现，为新闻推荐研究提供基准。实验结果表明，利用NLP技术对新闻文章进行深入理解对新闻推荐具有重要意义。有效的文本表示方法和预先训练好的语言模型都有助于提高新闻推荐的性能。此外，适当地对用户兴趣进行建模也很有用。希望MIND可以作为新闻推荐的基准数据集，促进这一领域的研究。</p> 
<h3><a id="2__14"></a>2. 相关工作</h3> 
<h4><a id="21__15"></a>2.1 新闻推荐</h4> 
<p>新闻推荐旨在从大量候选新闻中寻找用户感兴趣的新闻文章(Das et al.，2007)。新闻推荐中存在两个重要的问题，即<strong>如何表示文本内容丰富的新闻文章</strong>，以及<strong>如何从用户之前的行为中建模用户对新闻的兴趣</strong>(Okura et al.， 2017)。传统的新闻推荐方法通常依靠<strong>特征工程来代表新闻文章和用户兴趣</strong>。例如，Li等人(2010)使用url和类别来表示新闻文章，使用人口统计数据、地理信息和从他们在雅虎上的消费记录推断出的行为类别来表示用户。</p> 
<p>近年来，<strong>一些基于深度学习的新闻推荐方法被提出</strong>，以端到端方式学习新闻文章的表示和用户兴趣(Okura et al., 2017; Wu et al., 2019a; An et al., 2019年)。例如，Okura等人(2017)使用<strong>去噪自动编码器模型</strong>表示来自新闻内容的新闻文章，使用<strong>GRU模型</strong>表示来自历史点击新闻文章的用户兴趣。他们在日本雅虎平台的实验显示，通过<strong>深度学习模型</strong>学习到的新闻和用户表示很有希望用于新闻推荐。Wang et al.(2018)提出通过结合从<strong>知识图谱</strong>中推导出的词向量和实体向量，利用<strong>CNN网络从新闻标题中学习知识感知</strong>的新闻表示。Wu et al. (2019a)提出了一种专注的<strong>多视角学习框架</strong>来表示标题、正文、类别等不同文本的新闻文章。他们使用了一个注意力模型，通过选择信息丰富的文章，从用户点击的新闻文章中推断出用户的兴趣。这些工作通常开发和验证的专有数据集，这是不可公开的，使它为其他研究人员验证这些方法和发展自己的方法困难。</p> 
<p>新闻推荐与NLP有着丰富的内在关联。首先，新闻是一种常见的文本形式，文本建模技术<strong>CNN和Transformer</strong>可以很自然地被用来表示新闻文章(Wu et al., 2019a; Ge et al., 2020)。其次，<strong>从之前点击的新闻文章</strong>中学习用户兴趣表示，与从句子中学习文档表示有相似之处。第三，新闻推荐可以<strong>表述为一个特殊的文本匹</strong>配问题，即在某个新闻阅读兴趣空间中，一篇<strong>候选新闻文章与一组之前点击过的新闻文章之间的匹配</strong>。因此，新闻推荐越来越受到NLP社区的关注(An et al., 2019; Wu et al.2019c)。</p> 
<h4><a id="22__22"></a>2.2 现有数据集</h4> 
<p><img src="https://images2.imgbox.com/4b/39/DLIMFlxC_o.png" alt="在这里插入图片描述"><br> 新闻推荐的公共数据集很少，总结见表1。Kille等人(2013)通过收集在13个德国新闻门户网站上发表的新闻文章以及用户对其的点击日志，构建了<strong>Plista数据集</strong>，它包含70353篇新闻文章和1095323次点击事件。该数据集中的新闻文章是德语的。Gulla等人(2017)在十周内发布了<strong>Adressa数据集</strong>，该数据来源于Adresseavisen网站的日志。它拥有48486篇新闻文章，308w用户和2722w点击事件。每个点击事件包含几个特性，如会话时间、新闻标题、新闻类别和用户ID。每个新闻文章都与一些详细信息相关联，如作者、实体和主体。这个数据集中的新闻文章是挪威语的。Moreira等人(2018)从巴西热门新闻门户Globo.com构建<strong>Globo新闻推荐</strong>数据。这个数据集包含大约31w个用户，46000篇新闻文章和300万次点击记录。每个单击记录都包含用户ID、新闻ID和会话时间等字段。每个新闻文章都有ID、分类、发布者、创建时间以及由一个在news metadata分类任务中预训练的神经网络得到的词向量。<strong>Yahoo数据集</strong>用于基于会话的新闻推荐。它包含14180篇新闻文章和34022次点击事件。每个新闻文章都由单词ID表示，不提供原始新闻文本。该数据集中的用户数量未知，因为没有用户ID。</p> 
<h3><a id="3_MIND_26"></a>3. MIND数据集</h3> 
<h4><a id="31__27"></a>3.1 数据集构建</h4> 
<p>本文构建了MIND数据集(MIcrosoft News Dataset)。它是从Microsoft News的用户行为日志中收集的。随机抽取了100万用户，他们在2019年10月12日至11月22日的6周内至少有5条新闻点击记录。为了保护用户隐私，将每个用户安全地散列到一个匿名ID中。我们收集了这些用户在此期间的行为日志，并将其格式化为impression logs。<strong>impression logs 记录</strong>用户在特定时间访问新闻网站主页时<strong>显示给用户的新闻文章</strong>，以及<strong>用户对这些新闻文章的点击行为</strong>。由于在新闻推荐中，通常根据用户之前的行为推断出的个人兴趣来预测用户是否会点击候选新闻文章，因此将用户的新闻点击历史记录添加到他们的impression logs中，构造带标签的样本（labeled sample），用于训练和验证新闻推荐模型。</p> 
<blockquote> 
 <p>每个带标签的示例的格式是<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          [ 
         
        
          u 
         
        
          I 
         
        
          D 
         
        
          , 
         
        
          t 
         
        
          , 
         
        
          C 
         
        
          l 
         
        
          i 
         
        
          c 
         
        
          k 
         
        
          H 
         
        
          i 
         
        
          s 
         
        
          t 
         
        
          , 
         
        
          I 
         
        
          m 
         
        
          p 
         
        
          L 
         
        
          o 
         
        
          g 
         
        
          ] 
         
        
       
         [uID, t, ClickHist, ImpLog] 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">[</span><span class="mord mathdefault">u</span><span class="mord mathdefault" style="margin-right: 0.07847em;">I</span><span class="mord mathdefault" style="margin-right: 0.02778em;">D</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathdefault">t</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathdefault" style="margin-right: 0.07153em;">C</span><span class="mord mathdefault" style="margin-right: 0.01968em;">l</span><span class="mord mathdefault">i</span><span class="mord mathdefault">c</span><span class="mord mathdefault" style="margin-right: 0.03148em;">k</span><span class="mord mathdefault" style="margin-right: 0.08125em;">H</span><span class="mord mathdefault">i</span><span class="mord mathdefault">s</span><span class="mord mathdefault">t</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathdefault" style="margin-right: 0.07847em;">I</span><span class="mord mathdefault">m</span><span class="mord mathdefault">p</span><span class="mord mathdefault">L</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right: 0.03588em;">g</span><span class="mclose">]</span></span></span></span></span>，其中<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          u 
         
        
          I 
         
        
          D 
         
        
       
         uID 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault">u</span><span class="mord mathdefault" style="margin-right: 0.07847em;">I</span><span class="mord mathdefault" style="margin-right: 0.02778em;">D</span></span></span></span></span>是用户的匿名ID， <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          t 
         
        
       
         t 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.61508em; vertical-align: 0em;"></span><span class="mord mathdefault">t</span></span></span></span></span>是这个impression的时间戳。<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          C 
         
        
          l 
         
        
          i 
         
        
          c 
         
        
          k 
         
        
          H 
         
        
          i 
         
        
          s 
         
        
          t 
         
        
       
         ClickHist 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.07153em;">C</span><span class="mord mathdefault" style="margin-right: 0.01968em;">l</span><span class="mord mathdefault">i</span><span class="mord mathdefault">c</span><span class="mord mathdefault" style="margin-right: 0.03148em;">k</span><span class="mord mathdefault" style="margin-right: 0.08125em;">H</span><span class="mord mathdefault">i</span><span class="mord mathdefault">s</span><span class="mord mathdefault">t</span></span></span></span></span>是该用户先前单击的新闻文章的ID列表(按单击时间排序)。<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          I 
         
        
          m 
         
        
          p 
         
        
          L 
         
        
          o 
         
        
          g 
         
        
       
         ImpLog 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.87777em; vertical-align: -0.19444em;"></span><span class="mord mathdefault" style="margin-right: 0.07847em;">I</span><span class="mord mathdefault">m</span><span class="mord mathdefault">p</span><span class="mord mathdefault">L</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right: 0.03588em;">g</span></span></span></span></span>包含显示在此impression中的新闻文章的id和指示是否点击它们的标签，即<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          [ 
         
        
          ( 
         
        
          n 
         
        
          I 
         
         
         
           D 
          
         
           1 
          
         
        
          , 
         
        
          l 
         
        
          a 
         
        
          b 
         
        
          e 
         
         
         
           l 
          
         
           1 
          
         
        
          ) 
         
        
          , 
         
        
          ( 
         
        
          n 
         
        
          I 
         
         
         
           D 
          
         
           2 
          
         
        
          , 
         
        
          l 
         
        
          a 
         
        
          b 
         
        
          e 
         
         
         
           l 
          
         
           2 
          
         
        
          ) 
         
        
          , 
         
        
          … 
          
        
          ] 
         
        
       
         [(nID_1, label_1), (nID_2, label_2), \dots] 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">[</span><span class="mopen">(</span><span class="mord mathdefault">n</span><span class="mord mathdefault" style="margin-right: 0.07847em;">I</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.02778em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathdefault" style="margin-right: 0.01968em;">l</span><span class="mord mathdefault">a</span><span class="mord mathdefault">b</span><span class="mord mathdefault">e</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.01968em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mopen">(</span><span class="mord mathdefault">n</span><span class="mord mathdefault" style="margin-right: 0.07847em;">I</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.02778em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathdefault" style="margin-right: 0.01968em;">l</span><span class="mord mathdefault">a</span><span class="mord mathdefault">b</span><span class="mord mathdefault">e</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.01968em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mclose">]</span></span></span></span></span>，其中<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          n 
         
        
          I 
         
        
          D 
         
        
       
         nID 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault">n</span><span class="mord mathdefault" style="margin-right: 0.07847em;">I</span><span class="mord mathdefault" style="margin-right: 0.02778em;">D</span></span></span></span></span>为新闻文章ID, <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          l 
         
        
          a 
         
        
          b 
         
        
          e 
         
        
          l 
         
        
       
         label 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.01968em;">l</span><span class="mord mathdefault">a</span><span class="mord mathdefault">b</span><span class="mord mathdefault">e</span><span class="mord mathdefault" style="margin-right: 0.01968em;">l</span></span></span></span></span>为点击标签（1表示点击，0表示未点击）。</p> 
 <p>使用最后一周的样本进行测试，使用第五周的样本进行训练。对于训练集中的样本，使用前四周的点击行为来构建新闻的点击历史。对于测试集中的示例，提取新闻点击历史记录的时间段是前五周。我们只保留了带有非空新闻点击历史记录的示例。在训练数据中，我们使用第五周最后一天的样本作为验证集。</p> 
</blockquote> 
<p><img src="https://images2.imgbox.com/49/6c/Qzp3eRTi_o.png" alt="在这里插入图片描述"><br> MIND数据集中的每一篇新闻文章都包含一个新闻ID、一个title、一个abstract、一个body和一个category label，比如人工打标label的“Sports”。此外，我们发现这些新闻文本包含丰富的实体。例如，在图1所示的新闻标题中，“Mike Tomlin: Steelers ‘accept responsibility’ for role in brawl with Browns”，“Mike Tomlin”是一个person entity，“Steelers”和“Browns”是美式足球队的entity。为了便于知识感知型新闻推荐的研究，我们将新闻文章的title、abstract和body中的实体提取到MIND数据集中，并使用内部的NER和实体链接工具链接到WikiData中的实体。还从WikiData中提取了这些实体的知识三元组，并使用TransE (Bordes et al.， 2013)方法学习实体和关系的embeddings。这些实体、知识三元组以及实体和关系embeddings也包含在MIND数据集中。</p> 
<h4><a id="32_httpsimgblogcsdnimgcn960f51b399924d89ba76483261108ca1pngpic_centerhttpsimgblogcsdnimgcn974f595f047440199da6ac3ee1a69348pngpic_center_36"></a>3.2 数据分析<img src="https://images2.imgbox.com/7b/f9/2cnrxs2c_o.png" alt="在这里插入图片描述"><img src="https://images2.imgbox.com/06/36/QPoEKKoT_o.png" alt="在这里插入图片描述"></h4> 
<p>MIND数据集的详细统计如表2和图2所示。该数据集包含10万用户和16万篇新闻文章。在训练集中有218万个样本，验证集中有36万个样本，测试集有234万样本，这样可以增强数据密集型新闻推荐模型的训练。图2(a)、(b)、（c）展示了新闻title、abstract、body的长度分布。我们可以看到，新闻标题通常很短，平均长度只有11.52个字。相比之下，新闻摘要和正文要长得多，可以包含更丰富的新闻内容信息。因此，整合title、abstract、body等不同类型的新闻信息有助于更好地理解新闻文章。图2(d)显示了新闻文章的生存时间分布。这里使用新闻文章在数据集中首次出现和最后一次出现之间的时间间隔估计新闻文章的生存时间。我们发现，超过84.5%的新闻文章存活时间不到两天。<strong>这是由于新闻信息的性质，新闻媒体总是追求最新的新闻</strong>，现有的新闻文章很快就会过时。因此，冷启动问题是新闻推荐中常见的现象，传统的基于id的推荐系统(Koren, 2008)并不适合这项任务。使用文本内容表示新闻文章对新闻推荐至关重要。</p> 
<h3><a id="4__38"></a>4. 方法</h3> 
<h4><a id="41__39"></a>4.1 通用推荐方法</h4> 
<ul><li><strong>LibFM</strong>：2012年Rendle发提出LibFM是一个基于因子分解机（factorization machine）的经典推荐方法。除了用户ID和新闻ID之外，我们还使用从先前点击的新闻和候选新闻中提取的内容特性作为附加特性来表示用户和候选新闻。</li><li><strong>DSSM</strong>：2013年Huang等人提出深度结构化语义模型（deep structured semantic model），采用三格哈希（tri-gram hashes）和多前馈神经网络进行查询文档匹配。我们使用从先前点击的新闻中提取的内容特性作为查询，从候选新闻中提取的内容特性作为文档。</li><li><strong>Wide&amp;Deep</strong>：2016年Cheng等人提出了一种宽线性变换通道和深神经网络通道（wide linear transformation channel）的双通道神经网络推荐方法。我们为两个channel使用相同的用户和候选新闻内容特性。</li><li><strong>DeepFM</strong>：2017年Guo等人提出另一种流行的神经推荐方法是将深度神经网络和分解机结合起来。为两个组件提供相同的用户和候选新闻内容特性。</li></ul> 
<h4><a id="42__44"></a>4.2 新闻推荐方法</h4> 
<ul><li> <p><strong>DFM</strong>：2018年Lian等人提出深度融合模型(deep fusion model)，一种新闻推荐方法，它采用inception网络将不同深度的神经网络结合起来，捕捉特征之间复杂的交互作用。我们使用了与上述方法相同的用户和候选新闻特性。</p> </li><li> <p><strong>GRU</strong>：2017年Okura等人利用自动编码器从新闻内容中学习潜在的新闻表示，并利用GRU网络从点击的新闻序列中学习用户表示。</p> </li><li> <p><strong>DKN</strong>：2018年Wang等人提出一种知识感知（knowledge-aware）的新闻推荐方法。使用CNN从包含word embedding和entity embedding的新闻标题中提取新闻的representaion(从知识图中推断)，并根据候选新闻与之前点击的新闻之间的相似性来学习用户representaion。</p> </li><li> <p><strong>NPA</strong>：2019年Wu等人提出一种具有个性化注意机制的神经新闻推荐方法，根据用户偏好选择重要词汇和新闻文章，以获取更多信息的新闻和用户表示。</p> </li><li> <p><strong>NAML</strong>：2019年Wu等人提出一种专注多视角学习的神经新闻推荐方法，将不同种类的新闻信息合并到新闻文章的表示中。</p> </li><li> <p><strong>LSTUR</strong>：2019年An等人提出一种具有长期和短期用户兴趣的神经新闻推荐方法。它利用GRU从用户最近点击的新闻中塑造短期用户兴趣，从整个点击历史中塑造长期用户兴趣。</p> </li><li> <p><strong>NRMS</strong>：2019年Wu等人利用多头自注意（multi-head self-attention）从新闻文本中的单词中学习新闻表示，从已点击的新闻文章中学习用户表示。</p> </li></ul> 
<h3><a id="5__59"></a>5. 个人公众号：四一五学习笔记</h3> 
<p>由于CSDN一直把公众号的二维码当做违规处置，故增加一个标题。<br> 公众号会不定期更新机器学习文章笔记。感兴趣的同学可以搜索【四一五学习笔记】进行关注。😁<br> <img src="https://images2.imgbox.com/ed/83/ZafbjUCS_o.png" alt="在这里插入图片描述"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/4dcd98323496289ee650c411813d32a0/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">自然语言处理—RNN循环神经网络</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/5f2884f93beb0fe749c959809df6da15/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">详解 c语言中的操作符</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 追风少年的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>