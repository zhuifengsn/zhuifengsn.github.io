<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>人工智胀 | 堂妹教你用KNN算法筛选约会对象 - 追风少年的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="人工智胀 | 堂妹教你用KNN算法筛选约会对象" />
<meta property="og:description" content="写在前面 咕咕咕~
堂妹又回来啦😀
早前读者群小伙伴希望我出「机器学习入门系列」，这就来安排~ 接下来，我会按照自己的节(慢)奏(慢)推出机器学习入门算法系列文章。
我给这个系列取名为「人工智&#34;胀&#34;」，是人工智「能」和人工智「障」的兄弟，
我知道我的读者很多不是这个领域的同学，没事儿，
我们边玩边学，希望轻松看文的同时有所收获。
当然，如果真的是想学技术，建议文档和源码。
带有情绪的技术文，本身就是图个乐~
开篇之作，我们先看个简易入门款：KNN。
KNN全称是 K Nearest Neighbors，直译就是「K个最近的邻居」。
理解一下这个表述，要「K个最近的邻居干啥呢」？
邻居是相对的概念，相对应的那个主体就是要讨论的对象。
换句话：“近朱者赤，近墨者黑”。
所以，用近邻对象的特征来表示主体的特征，即K近邻算法。
举个极端例子：
当你想要了解某个人的特征，可以参考他身边人的特征，
他身边人的性格也就大致上代表了他的性格。
这里面有两个问题：
问题1：参考几个人？
问题2：怎么衡量&#34;身边&#34;这个概念？
K值代表要参考的人数，当K为1，表示你只参考离它最近的☝️个人，显然很片面。
所以，K值怎么选得到的结果最准确，这是个学问，后面我们慢慢来看。
&#34;身边&#34;这个概念，可以量化成多种度量方式，最朴素的做法：欧氏距离。
一句总结：通过计算一群人和目前对象各自的欧氏距离，得到与目标对象距离最近的K个人，这K个人中出现频率最高的性格标签即为目标对象的性格标签。
so，快看看你身边有多少🦊朋🐕友（doge）。
大概原理如上，光说不练假把式。
我直接在数据集上验证一下这个算法的效果。
我在某数据集上进行了算法实现，
并给出了把准确率从76%提升到100%的优化思路和实现方案。
代码链接已公开在文末，开箱即用。
实验背景 数据来源：某约会网站上多个候选对象的三种行为数据统计。
情境假设：我结合自己的要求，利用已有数据构建算法模型。当有新的候选人来的时候，直接丢进模型，我就知道是不是我喜欢的类型，如果是，再考虑要不要约出来喝咖啡。节省大家时间，真是太机智了😉。
堂妹声明：以下措词不要代入我本人，结合场景介绍算法，而已。
我将候选人分为三大类：
类别标签一：不喜欢的人(didnotlike)
类别标签二：魅力一般的人(smallDoses)
类别标签三：极具魅力的人(largeDoses)
而三大类别对应的考核标准如下：
特征一：每年乘坐飞机的里程数
特征二：玩视频游戏所消耗的时间百分比
特征三：每周消费的冰淇凌公升数
我们来做一个可视化，对整体的数据分布有个认识~
从上面这幅图我们有个初步印象：如果考虑飞机里程和游戏时间这两个因素，🛬里程多的大概率被划分到不喜欢的一类。
如果考虑飞机里程和喜欢吃冰激凌的程度两个因素，吃多少冰激凌不影响，有适当外出习惯的同学最佳。
再看看玩游戏时间和冰激凌两个因素，从上面这幅图没有看出什么明显信息。
因此，初步结论：
太喜欢往外跑的，不考虑；
冰激凌的摄入量对选择影响不大。
带着这样的初步判断，我们实现一下算法。
算法实现 算法流程：
1. 准备待评测的数据
2. 计算待评测数据与每个已知数据的欧式距离
3. 将距离的结果排序，取前K个最近距离的类别标签
4. 统计类别标签出现的频率
5. 出现频率最高的类别标签即为待测数据的类别标签。
根据这个流程，我们实现KNN算法如下：
def classify(testData, knownData, labels, k): &#39;&#39;&#39; testData代表待测数据 knownData代表已知数据 labels代表已知数据的标签 k代表统计前k个数据 &#39;&#39;&#39; DataSetSize = knownData." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://zhuifengsn.github.io/posts/05463addbc04c10c57ec6f9654c8a8e9/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-11-15T22:04:23+08:00" />
<meta property="article:modified_time" content="2021-11-15T22:04:23+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="追风少年的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">追风少年的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">人工智胀 | 堂妹教你用KNN算法筛选约会对象</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2><span style="color:#ff9900;">写在前面</span></h2> 
<blockquote> 
 <p>咕咕咕~</p> 
 <p>堂妹又回来啦😀</p> 
 <p>早前读者群小伙伴希望我出「机器学习入门系列」，这就来安排~ </p> 
 <p>接下来，我会按照自己的节(慢)奏(慢)推出机器学习入门算法系列文章。</p> 
 <p>我给这个系列取名为「人工智"胀"」，是人工智「能」和人工智「障」的兄弟，</p> 
 <p>我知道我的读者很多不是这个领域的同学，没事儿，</p> 
 <p>我们边玩边学，希望轻松看文的同时有所收获。</p> 
 <p>当然，如果真的是想学技术，<span style="color:#ff9900;"><strong>建议文档和源码</strong>。</span></p> 
 <p>带有情绪的技术文，本身就是图个乐~</p> 
</blockquote> 
<p></p> 
<p>开篇之作，我们先看个简易入门款：KNN。</p> 
<p>KNN全称是 K Nearest Neighbors，直译就是「K个最近的邻居」。</p> 
<p>理解一下这个表述，要「K个最近的邻居干啥呢」？</p> 
<p>邻居是相对的概念，相对应的那个主体就是要讨论的对象。</p> 
<p>换句话：“<span style="color:#ff9900;"><strong>近朱者赤，近墨者黑</strong></span>”。</p> 
<p><strong><span style="color:#ff9900;">所以，用近邻对象的特征来表示主体的特征，即K近邻算法</span>。</strong></p> 
<p>举个极端例子：</p> 
<p>当你想要了解某个人的特征，可以参考他身边人的特征，</p> 
<p>他身边人的性格也就大致上代表了他的性格。</p> 
<p>这里面有两个问题：</p> 
<p>问题1：参考几个人？</p> 
<p>问题2：怎么衡量"身边"这个概念？</p> 
<p>K值代表要参考的人数，当K为1，表示你只参考离它最近的☝️个人，显然很片面。</p> 
<p>所以，K值怎么选得到的结果最准确，这是个学问，后面我们慢慢来看。</p> 
<p>"身边"这个概念，可以量化成多种度量方式，最朴素的做法：欧氏距离。</p> 
<p><strong>一句总结：</strong>通过计算一群人和目前对象各自的欧氏距离，得到与目标对象距离最近的K个人，这K个人中出现频率最高的性格标签即为目标对象的性格标签。</p> 
<p>so，快看看你身边有多少🦊朋🐕友（doge）。</p> 
<p>大概原理如上，光说不练假把式。</p> 
<p>我直接在数据集上验证一下这个算法的效果。</p> 
<p>我在某数据集上进行了算法实现，</p> 
<p>并给出了把<span style="color:#ff9900;"><u><em><strong>准确率从76%提升到100%</strong><strong>的优化思路和实现方案</strong></em></u></span>。</p> 
<p>代码链接已公开在文末，开箱即用。</p> 
<h2></h2> 
<h2><span style="color:#ff9900;"><strong>实验背景</strong></span></h2> 
<ul><li> <p>数据来源：某约会网站上多个候选对象的三种行为数据统计。</p> </li><li> <p>情境假设：我结合自己的要求，利用已有数据构建算法模型。当有新的候选人来的时候，直接丢进模型，我就知道是不是我喜欢的类型，如果是，再考虑要不要约出来喝咖啡。节省大家时间，真是太机智了😉。</p> </li><li> <p>堂妹声明：<u>以下措词不要代入我本人，结合场景介绍算法，而已</u>。</p> <p></p> </li></ul> 
<p>我将候选人分为三大类：</p> 
<ul><li> <p>类别标签一：不喜欢的人(didnotlike)</p> </li><li> <p>类别标签二：魅力一般的人(smallDoses)</p> </li><li> <p>类别标签三：极具魅力的人(largeDoses)</p> </li></ul> 
<p>而三大类别对应的考核标准如下：</p> 
<ul><li> <p>特征一：每年乘坐飞机的里程数</p> </li><li> <p>特征二：玩视频游戏所消耗的时间百分比</p> </li><li> <p>特征三：每周消费的冰淇凌公升数</p> </li></ul> 
<p></p> 
<p>我们来做一个可视化，对整体的数据分布有个认识~</p> 
<p style="text-align:center;"><img alt="图片" src="https://images2.imgbox.com/84/21/pE7z0JKS_o.png"></p> 
<p> 从上面这幅图我们有个初步印象：如果考虑飞机里程和游戏时间这两个因素，🛬里程多的大概率被划分到不喜欢的一类。</p> 
<p style="text-align:center;"><img alt="图片" src="https://images2.imgbox.com/53/b3/4BcWyY43_o.png"></p> 
<p> 如果考虑飞机里程和喜欢吃冰激凌的程度两个因素，吃多少冰激凌不影响，有适当外出习惯的同学最佳。</p> 
<p style="text-align:center;"><img alt="图片" src="https://images2.imgbox.com/3a/7f/Oemsf7NE_o.png"></p> 
<p>再看看玩游戏时间和冰激凌两个因素，从上面这幅图没有看出什么明显信息。</p> 
<p>因此，初步结论：</p> 
<ul><li> <p>太喜欢往外跑的，不考虑；</p> </li><li> <p>冰激凌的摄入量对选择影响不大。</p> </li></ul> 
<p> 带着这样的初步判断，我们实现一下算法。</p> 
<h2><span style="color:#ff9900;"> 算法实现</span></h2> 
<p>算法流程：</p> 
<blockquote> 
 <p>1. 准备待评测的数据<br> 2. 计算待评测数据与每个已知数据的欧式距离<br> 3. 将距离的结果排序，取前K个最近距离的类别标签<br> 4. 统计类别标签出现的频率<br> 5. 出现频率最高的类别标签即为待测数据的类别标签。</p> 
</blockquote> 
<p>根据这个流程，我们实现KNN算法如下：</p> 
<pre><code class="language-python">def classify(testData, knownData, labels, k): 
    '''
    testData代表待测数据
    knownData代表已知数据
    labels代表已知数据的标签
    k代表统计前k个数据
    '''
      DataSetSize = knownData.shape[0]        
      #计算输入数据与已知数据的欧式距离
      diffMat = np.tile(testMat,(dataSetSize,1)) - dating #距离相减
      sqDiffMat = diffMat**2    #求平方
      sqDistances = sqDiffMat.sum(axis=1)    #求和
      sqDistances = sqrt(sqDistances)    #求根号
      sortedDisIndices = (sqDistances).argsort() #将距离排序
      classCount = {}
      for i in range(k):
            votelLabel = labels[sortedDisIndices[i]]  #取前K个标签
            classCount[votelLabel] = classCount.get(votelLabel,0) + 1 #统计标签出现的频率
            sortedClassCount = sorted(classCount.items(),key=operator.itemgetter(1),reverse=True) #将标签根据频率排序
      return sortedClassCount[0][0] #返回频率最高的标签</code></pre> 
<p>我们将现有数据分为两部分，一部分作为已知数据用于计算得到类别结果，俗称预测结果，一部分用于验证结果的准确性。</p> 
<p>代码片段如下所示：</p> 
<pre><code class="language-python">from dataload import Data  # 读取数据函数，自己定义(github中有写)
dataset = Data()
ratio = 0.1 #准备划分10%数据测试
k = 10 #取前K个点的测试标签
datingmat, datinglabel = dataset.load_data('data.txt')  #读取数据和标签,读取为矩阵形式

accuracy = result_knn(k,ratio,datinglabel,datingmat)  #输入测试数据
print('accuracy rate: ',acccuracy）  #输出测试准确率

def result_knn(k,ratio,datinglabel,datingmat):
    '''
    计算测试数据的准确率
    '''
    count = 0  #计算测试准确的个数
    test_num = int(ratio * len(datinglabel))  #分出前10%当作未知数据进行测试
    for i in range(test_num):  #测试未知数据
        class_result =classify(datingmat[i,:],datingmat[test_num:,:],datinglabel[test_num:], k)  #测试第i个数据的标签
        if class_result == self.datinglabel[i]:  #判断测试是否正确
            count += 1
    return count/test_num</code></pre> 
<p>将通过KNN算法计算得出的结果和其对应的真实标签计算准确率，</p> 
<pre><code>$ python test_knn.py

The best accuracy is : 76.0%</code></pre> 
<p>只有<span style="color:#ff9900;"><strong>76%</strong><strong>，</strong><strong>这个不太行呀🙃。</strong></span></p> 
<p>我们再来分析一下数据，发现不同特征的数据分布差别很大，如下表：</p> 
<table align="center" border="1" cellpadding="1" cellspacing="1" style="width:500px;"><tbody><tr><td></td><td>  里程数</td><td> 游戏时间</td><td>冰淇淋量(升)</td></tr><tr><td>最大值</td><td>91273.0</td><td>20.1</td><td>1.7</td></tr><tr><td>最小值</td><td>0.0</td><td>0.0</td><td>0.0</td></tr></tbody></table> 
<p>有过先验知识的同学一眼就能看出来，在计算距离的过程中，里程数变化较小的幅度就能引起结果的巨变，所以相当于前面的计算方式无形之中给了里程数一个<span style="color:#ff9900;"><strong>高置信度</strong></span><strong>。</strong></p> 
<blockquote> 
 <p>置信度：指结果依赖某个特征的程度，高置信度也就是说这个特征对结果产生的影响比其他的特征影响大。</p> 
</blockquote> 
<p> 这时候就想到经典的规范数据分布的操作：归一化。</p> 
<p>我们将所有的特征进行归一化，给每个特征相同的置信度，公式如下：                       <img alt="matrix= \frac{matrix-min\left ( matrix \right )}{max\left ( matrix \right )-min\left ( matrix \right )}" class="mathcode" src="https://images2.imgbox.com/79/20/Qr8KYQUN_o.png"><br> 然后再次进行进行计算</p> 
<pre><code>$ python test_knn.py --autonorm

The best accuracy is : 94.0%</code></pre> 
<p>结果显示<span style="color:#ff9900;"><strong>准确率94%</strong></span>，论归一化的重要性！</p> 
<p>到这儿关于KNN基本的流程差不多快结束了，我们回到最开始的问题：</p> 
<ul><li> <p>K值选多少合适？</p> </li><li> <p>怎么衡量距离近？</p> </li></ul> 
<p>这两个方面涉及KNN的优化，可选方案如下：</p> 
<h4><span style="color:#ff9900;"><strong>1. 选择最优K值</strong></span></h4> 
<p></p> 
<p>如开头所说，如果K=1，也就是只考虑最相似的一个数据，容易受噪点的影响。</p> 
<p>如果K值过大，那可能会引入过的干扰类别标签，导致决策失误。</p> 
<p>这时候我们可以通过「交叉验证法」来找到最佳K值。</p> 
<p>交叉验证法是个比较常见的方法了，周志华老师的西瓜书前几章就有讲。</p> 
<blockquote> 
 <p>交叉验证法：将数据分为n组，循环选取某一组数据作为未知数据进行测试，其他数据作为已知数据计算结果，循环n次。</p> 
</blockquote> 
<p>在这个算法中，每次取不同的K值，用交叉验证法验证N次，取N次的平均准确率作为对应K值的测试结果，直到找到最好的K值。</p> 
<p><span style="color:#ff9900;"><strong>2. 对距离函数做加权处理</strong></span></p> 
<p>当我们计算欧式距离时，离我们更近的对象是不是可以更具有影响力？</p> 
<p>而KNN将前K个距离的影响看作是相同的(计数时都看做加1)</p> 
<p>那我们为每个点的距离增加一个权重，使得距离近的点可以得到更大的权重</p> 
<p>那结果会不会更好一点？</p> 
<p>加权处理的方法有很多，例如：反函数、高斯函数:</p> 
<p><strong>反函数</strong>：</p> 
<p>该方法就是返回距离的倒数，公式如下：</p> 
<p><img alt="W = \frac{1}{D+const}" class="mathcode" src="https://images2.imgbox.com/c5/30/6pHKsjrb_o.png"></p> 
<p></p> 
<p>其中W是权重，D是距离，这种方法能够给近邻一个大的权重，但是有时候会使算法对噪声数据变得特别敏感，例如某个噪声数据距离测试点特别近时，测得的权重很大，所以我们加入const作为约束，缓解权重过大的情况。 </p> 
<p><strong>高斯函数</strong>：</p> 
<p>高斯函数，公式如下：</p> 
<p><img alt="f\left ( x \right )=ae^{-\frac{\left ( x-b \right )^{2}}{2c^{2}}}" class="mathcode" src="https://images2.imgbox.com/30/9f/VukVIrSU_o.png"></p> 
<p>其中a是曲线的高度、b是曲线中心在x轴的偏移、c是半峰宽度(函数峰值一半处相距宽度)，高斯函数相较于反函数在权重分配上更合理。</p> 
<p>例如当两个特征特别近时，反函数分配的权重可能是无穷大，而高斯函数的权重最大为a，结果更可控。</p> 
<p> <strong>3.优化特征的置信度</strong></p> 
<p></p> 
<p>上述做了归一化之后，将每个特征都看做是相同的置信度，但这样会把一些隐藏信息给忽略掉，过犹不及~</p> 
<p>例如：冰淇淋的公升数变化并没有引起结果很大的变化，我们是否可以考虑将这个特征的置信度给减弱呢？</p> 
<p>在这里，我们直接用第三种优化方案进行实践，一起来看下优化有没有效果。</p> 
<p>我们采用网格法搜索🔍特征的最佳置信度：</p> 
<blockquote> 
 <p>网格法：给定参数的取值范围，根据指定规则生成一组数据。遍历数据集合得到最佳参数。</p> 
 <p>在这个例子中，就是构造三个特征参数列表，遍历获得对应每个特征的最佳组合。</p> 
</blockquote> 
<p> 代码节选如下：</p> 
<pre><code class="language-python">import numpy as np
airplane_mileage, game_rate, ice_cream = [i for i in range(0，2，0.1)], [y for y in range(0，2，0.1)], [z for z in range(0，2，0.1)]  #指定超参数范围
max_accuracy = 0.0
datingmat = autoNorm(datingmat) #归一化数据(自己定义的函数)
for am in airplane_mileage:  #三组参数测试，得出最佳参数组合
  for gr in game_rate:
    for ic in ice_cream:
      dating = (np.array([am,gr,ic]).reshape(3,1))*datingmat  #获取处理后的数据
      accuracy = result_knn(k,ratio,datinglabel,datingmat)
      if accuracy &gt;= max_accuracy:
        max_accuracy = accuracy #保存测试的最佳准确率
        parameter_gr, parameter_am, parameter_ic = gr, am, ic #保存最佳特征置信度的参数
print('Accuracy rate : {}\n Best airplane_mileage confidence : {} \n Best game_rate confidence : {} \n Best ice_cream : {}'.format(max_accuracy,parameter_am,parameter_gr,parameter_ic))</code></pre> 
<p>最终我们得到的特征的最佳置信度为 <span style="color:#ff9900;"><strong>0.21,1.99,0.08</strong></span></p> 
<pre><code>$ python test_knn.py --autonorm --grid_serach

The best accuracy is : 100.0%
The best confidence is :
airplane mileage:0.21  game rate:1.99  ice cream:0.08</code></pre> 
<p>准确率从95%提升到<span style="color:#ff9900;"><strong>100%!!!! ✿✿ヽ(°▽°)ノ✿</strong></span></p> 
<p>我们来看下这组最优的置信度：</p> 
<ul><li> <p>里程置信度为0.21，处于中间水平 </p> </li><li> <p>游戏时间占比占据最高置信度</p> </li><li> <p>冰淇淋公升数置信度几乎为零</p> </li></ul> 
<p>回想一下前面我们的预判，冰激凌确实对结果没什么影响~</p> 
<p><strong>那为什么置信度高低会影响准确率呢？</strong></p> 
<p>大家可以考虑这么个例子：</p> 
<p>假设考虑x(0,0)和y(1,1)两点，计算欧式距离为<img alt="\sqrt{2}" class="mathcode" src="https://images2.imgbox.com/b8/5f/MsAM9wqN_o.png">，这时候每一个维度的置信度为1，如果将这两个维度的置信度变成(2,1)，那么欧式距离为<img alt="\sqrt{5}" class="mathcode" src="https://images2.imgbox.com/e4/bb/JRbZmNd0_o.png">，当置信度变成(0.5,1)时，欧式距离为<img alt="\sqrt{1.25}" class="mathcode" src="https://images2.imgbox.com/fe/3b/UnRoeMhX_o.png"></p> 
<p>显而易见，当位置不变，置信度改变时，距离的远近也会改变。某个维度置信度高，则结果对这个维度的信息更加敏感，反之亦然。</p> 
<p>在实际情况中，置信度则反映了结果对于每个维度的隐性敏感度。</p> 
<p>所以🦆，从数据整体分布来看。表面上我很在意外出✈️里程数，实际最在意的是男孩子玩游戏的时间，相信大部分👧女孩子都比较在意叭（doge）</p> 
<p>So，找对象，凭感觉是一方面，数据说话也是硬道理😁</p> 
<blockquote> 
 <p>堂妹在这儿用的是K=10，10%的数据测试准确率。大家可以尝试采用不同K值测试结果!!!</p> 
</blockquote> 
<h2> <span style="color:#ff9900;">结束啦</span></h2> 
<p>完结式撒花💐~</p> 
<p>完整链接戳：</p> 
<p>https://github.com/JDZTZ/machine_learning</p> 
<p>喜欢堂妹这个系列的小伙伴欢迎「分享、点赞」，</p> 
<p>鼓励堂妹多多产出，我在澡堂子里等你们🦆~</p> 
<p>什么？你还不知道怎么加入澡堂子？</p> 
<blockquote> 
 <p>微信扫一扫下方二维码，关注「九点澡堂子」</p> 
 <p>获取堂妹为你准备的求职大礼包、精品行业报告😘</p> 
 <p>回复「糖友福利」获取澡堂子专属福利💖</p> 
</blockquote> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/d1/f3/cFuKcBmg_o.png"></p> 
<p></p> 
<p></p> 
<p></p> 
<p style="text-align:center;"></p> 
<p></p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/53f10109e4321acb61bb414aa847a8c4/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">SpringBoot整合RabbitMQ与RabbitMQ高级特性</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/07b1e30e71372d598ed9cea45d33c298/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">小程序webview上传图片出现闪退</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 追风少年的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>